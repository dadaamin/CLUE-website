import{u as V,r as p,c as z,w as I,a as g,o as R,b as q,d as e,e as a,f as C,t as A,g as f}from"./index-dc5ef473.js";const N="/layout/images/clue-logo.png",Q="/layout/images/ume.png",j="/layout/images/NVIDIA_logo.svg",U={class:"w-full h-full flex flex-column align-items-center"},K={class:"w-full lg:w-7"},H={class:"card flex flex-wrap gap-3 justify-content-center header"},X=e("h1",{style:{"font-family":"'Google Sans', sans-serif"}},"CLUE: A Clinical Language Understanding Evaluation for LLMs",-1),J=e("img",{src:N,alt:"Image Description 1",class:"clue-logo"},null,-1),G=e("h4",null,"Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek",-1),W={href:"https://github.com/TIO-IKIM/CLUE",target:"_blank",rel:"noopener noreferrer"},Y={href:"https://arxiv.org/abs/2404.04067",target:"_blank",rel:"noopener noreferrer"},Z={id:"model-select",class:"card w-full my-3"},$=e("h3",null,"Motivation",-1),ee={class:"m-2"},te=e("p",{class:"m-2"}," Clinical texts often feature an irregular structure, abundant jargon and abbreviations that vary across specialties and regions, and are generally lengthier than the quiz questions used in standard datasets, resulting in a diverse and complex domain of texts. ",-1),ae=e("p",{class:"m-2"}," Moreover, recent research has uncovered substantial issues related to data contamination in LLM evaluations, which calls into question the validity of previous results. In light of these challenges, new strategies have been proposed to enhance the integrity and relevance of LLM testing. ",-1),se=e("p",{class:"m-2"}," Our project is committed to addressing these gaps by developing more robust and realistic medical evaluation methods for LLMs, ensuring they are better suited for real-world applications. This initiative represents a crucial step forward in understanding and improving the performance of LLMs in medical contexts. ",-1),ne={id:"model-select",class:"card w-full my-3"},oe=e("h3",null,"Model Selection",-1),ie={class:"card w-full my-3"},le=e("h3",{id:"results"},"Average Scores",-1),re={class:"card w-full my-3"},ce=e("h3",null,"Task Scores",-1),de={class:"w-full flex align-items-end align-content-start"},ue={class:"m-4"},me=e("div",{class:"flex-grow-1"},null,-1),he={class:"card w-full my-3"},pe=e("h3",null,"Task Descriptions",-1),ge=e("p",{class:"m-0"}," MeDiSumCode involves coding discharge summaries by assigning International Classification of Diseases (ICD-10) codes to diagnoses and procedures, a critical function for patient record management, billing, and statistics. It challenges language models to accurately extract diagnoses from lengthy, complex clinical texts and match them with the appropriate ICD-10 codes, which comprise over 70,000 options requiring extensive extensive knowledge of medical and coding systems. The task further tests the models' ability to integrate diagnosis identification with a detailed understanding of the ICD-10 structure for accurate code prediction. ",-1),fe=e("p",{class:"m-0"}," MeDiSumQA is a medical QA dataset that uses discharge summaries from MIMIC IV to generate question-answer pairs about a patient's hospital stay, focusing on extracting and querying key information. The questions are generated that involves identifying critical statements related to the patient’s medical history and hospitalization, then crafting questions that these statements can answer without. You can find a more detailed description in our paper. ",-1),ve=e("p",{class:"m-0"}," MeQSum comprises 1,000 consumer health inquiries from the U.S. National Library of Medicine that have been manually summarized by medical experts. This task challenges models to condense these often verbose and vague inquiries into concise, medically accurate summaries. The ability to effectively interpret and summarize patient inquiries, despite not involving clinical documents, is highly valuable for applications in healthcare settings. ",-1),Be=e("pre",{style:{"background-color":"#f5f5f5","overflow-x":"auto"}},[e("code",null,`@inproceedings{abacha2019summarization,
  title={On the summarization of consumer health questions},
  author={Abacha, Asma Ben and Demner-Fushman, Dina},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2228--2234},
  year={2019}
}`)],-1),Me=e("p",{class:"m-0"}," This downstream task, involves using the Subjective and Assessment sections of clinical notes, structured by the SOAP principle, to predict a patient's current health problems. The Objective section is excluded from inputs, while the Plan section provides the ground truth for the models' predictions. This task is a form of information extraction that challenges models to identify specific health issues from condensed, preprocessed clinical text. ",-1),be=e("pre",{style:{"background-color":"#f5f5f5","overflow-x":"auto"}},[e("code",null,`@inproceedings{gao2022summarizing,
  title={Summarizing patients’ problems from hospital progress notes using pre-trained sequence-to-sequence models},
  author={Gao, Yanjun and Miller, Timothy and Xu, Dongfang and Dligach, Dmitriy and Churpek, Matthew M and Afshar, Majid},
  booktitle={Proceedings of COLING. International Conference on Computational Linguistics},
  volume={2022},
  pages={2979},
  year={2022},
  organization={NIH Public Access}
}`)],-1),ye=e("p",{class:"m-0"}," MedNLI, derived from the MIMIC III dataset, features clinical notes from which sentences are extracted and used as premises for clinicians to generate three corresponding hypotheses: contradictory, neutral, and entailed. The core task involves predicting the logical relationship—whether it is contradiction, neutrality, or entailment—between each premise and its hypotheses. With its focus on short input lengths, MedNLI effectively measures a model's clinical reasoning capabilities without the complexity of processing long text inputs. ",-1),_e=e("pre",{style:{"background-color":"#f5f5f5","overflow-x":"auto"}},[e("code",null,`@article{romanov2018lessons,
	title = {Lessons from Natural Language Inference in the Clinical Domain},
	url = {http://arxiv.org/abs/1808.06752},
	abstract = {State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce {MedNLI} - a dataset annotated by doctors, performing a natural language inference task ({NLI}), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. {SNLI}) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.},
	journaltitle = {arXiv:1808.06752 [cs]},
	author = {Romanov, Alexey and Shivade, Chaitanya},
	urldate = {2018-08-27},
	date = {2018-08-21},
	eprinttype = {arxiv},
	eprint = {1808.06752},
}`)],-1),Le=e("p",{class:"m-0"}," The LongHealth dataset features 20 intricate fictional patient records to test language models on their ability to handle extended texts, as these models typically underperform with longer inputs. It tasks models with answering multiple-choice questions across various cognitive challenges like information extraction and negation, structured into three distinct evaluation sub-tasks. These tasks progressively increase in complexity by first focusing on relevant documents, then introducing extraneous information to filter out, and finally testing the model's propensity to incorrectly infer unprovided information. ",-1),xe=e("pre",{style:{"background-color":"#f5f5f5","overflow-x":"auto"}},[e("code",null,`@article{adams2024longhealth,
  title={LongHealth: A Question Answering Benchmark with Long Clinical Documents},
  author={Adams, Lisa and Busch, Felix and Han, Tianyu and Excoffier, Jean-Baptiste and Ortala, Matthieu and L{\\"o}ser, Alexander and Aerts, Hugo JWL and Kather, Jakob Nikolas and Truhn, Daniel and Bressem, Keno},
  journal={arXiv preprint arXiv:2401.14490},
  year={2024}
}`)],-1),Ie={class:"card w-full my-3"},Ce=e("h3",null,"BibTeX",-1),we=e("section",{class:"section",id:"BibTeX"},[e("div",{class:"container is-max-desktop content"},[e("p",null,"If you find our work useful, please cite our paper:"),e("pre",{style:{"background-color":"#f5f5f5","overflow-x":"auto"}},[e("code",null,`@article{dada2024clue,
  title={CLUE: A Clinical Language Understanding Evaluation for LLMs},
  author={Dada, Amin and Bauer, Marie and Contreras, Amanda Butler and Kora{\\c{s}}, Osman Alperen and Seibold, Constantin Marc and Smith, Kaleb E and Kleesiek, Jens},
  journal={arXiv preprint arXiv:2404.04067},
  year={2024}
}`)])])],-1),De={class:"card w-full my-3"},Ae=e("h3",null,"Contact",-1),ke=e("p",null,[C("Please reach out to "),e("a",{class:"link blue",href:"mailto:amin.dada@uk-essen.de"},"Amin Dada"),C(" if you have any comments, questions, or suggestions.")],-1),Se=e("div",{class:"image-container flex justify-content-center w-full"},[e("div",{class:"image-container flex justify-content-center w-full"},[e("img",{src:Q,alt:"Image Description 1",class:"ume-logo"}),e("img",{src:j,alt:"Image Description 2",class:"nvidia-logo"})])],-1),Te={__name:"Dashboard",setup(Fe){const{layoutConfig:k}=V();let d=getComputedStyle(document.documentElement);d.getPropertyValue("--text-color");let L=d.getPropertyValue("--text-color-secondary"),w=d.getPropertyValue("--surface-border");const D=["#BFD8D2","#F3E5AB","#B5B9FF","#FFC8A2","#ECCFCF","#C4C6E7","#E1E6FA","#D1E8E4","#FAE1DF","#EDE7B1","#C9C9FF","#FFDFD3","#A8E6CF","#D3A4FF","#D5ECC2","#FFF5BA","#FFD3B6","#DCEDC1","#F7D6E0","#FBE7C6","#A4B9EF","#FFABAB","#CABFAD","#B8E0F9","#9ED2C6","#FFDEB4","#FAE3D9","#B6CFB6","#AFCBEB","#C2C2F0"],u=p(["Mixtral-8x7B-Instruct-v0.1","Mixtral-8x22B-Instruct-v0.1","Llama3-OpenBioLLM-8B","Meta-Llama-3-8B-Instruct","Llama3-OpenBioLLM-70B","Meta-Llama-3-70B-Instruct","zephyr-7b-beta","Mistral-7B-Instruct-v0.1","BioMistral-7B-DARE","llama2-7b","llama2-70b","meditron-7b","meditron-70b","Mistral-7B-v0.1","BioMistral-7B"]),v=p(["Llama3-OpenBioLLM-8B","Meta-Llama-3-8B-Instruct","Llama3-OpenBioLLM-70B","Meta-Llama-3-70B-Instruct"]),B=u.value.reduce((t,s,i)=>(t[s]=D[i%D.length],t),{}),S={"Mixtral-8x7B-Instruct-v0.1":[64.44,41.78],"Mixtral-8x22B-Instruct-v0.1":[62.89],"Llama3-OpenBioLLM-8B":[46.78],"Meta-Llama-3-8B-Instruct":[60.9],"Llama3-OpenBioLLM-70B":[59.27],"Meta-Llama-3-70B-Instruct":[64.31],"zephyr-7b-beta":[60.2,35.25],"Mistral-7B-Instruct-v0.1":[57.96,27.9],"BioMistral-7B-DARE":[60.41,30.11],"llama2-7b":[33.76],"llama2-70b":[48.95],"meditron-7b":[26.32],"meditron-70b":[43.93],"BioMistral-7B":[57.77]},M={MeDiSumCode:{"zephyr-7b-beta":[2.31,12,71.27],"Mistral-7B-Instruct-v0.1":[.57,3.78,37.25],"BioMistral-7B-DARE":[1.2,6.66,56.04],"Mixtral-8x7B-Instruct-v0.1":[10.49,28.99,82.87],"Llama3-OpenBioLLM-8B":[.84,.48,51.16],"Meta-Llama-3-8B-Instruct":[3.95,17.55,61.93],"Llama3-OpenBioLLM-70B":[7.37,20.24,73.65],"Meta-Llama-3-70B-Instruct":[19.65,39.2,93.94],"Mixtral-8x22B-Instruct-v0.1":[15.95,35.96,79.84]},MeDiSumQA:{"zephyr-7b-beta":[18.2,21.3,8.91,66.27,19.06],"Mistral-7B-Instruct-v0.1":[18.78,21.5,10.05,66.71,18.63],"BioMistral-7B-DARE":[17.54,19.9,9.34,64.6,18.65],"Mixtral-8x7B-Instruct-v0.1":[25.54,29.26,13.89,69.57,24.47],"Llama3-OpenBioLLM-8B":[25.1,27.84,13.12,66.3,24.79],"Meta-Llama-3-8B-Instruct":[24.49,27.95,12.75,69.38,23.37],"Llama3-OpenBioLLM-70B":[24.43,27.73,13.29,68.98,24.5],"Meta-Llama-3-70B-Instruct":[27.24,30.95,14.24,70.71,25.84],"Mixtral-8x22B-Instruct-v0.1":[23.8,27.62,12.69,69.23,23.55]},MeQSum:{"zephyr-7b-beta":[7.16,8.58,2.89,37.5],"Mistral-7B-Instruct-v0.1":[21.85,25.26,11.17,66.15],"BioMistral-7B-DARE":[26.16,29.69,13.49,68.68],"Mixtral-8x7B-Instruct-v0.1":[32.47,36.38,16.86,72.8],"Llama3-OpenBioLLM-8B":[26.21,29.41,14.03,62.39],"Meta-Llama-3-8B-Instruct":[32.2,36.49,16.37,72.74],"Llama3-OpenBioLLM-70B":[30.72,34.31,15.55,71.99],"Meta-Llama-3-70B-Instruct":[36.57,40.2,19.3,75.74],"Mixtral-8x22B-Instruct-v0.1":[36.15,39.94,19.45,75.02]},"Problem Summary":{"zephyr-7b-beta":[5.97,7.35,2.11,59.45,9.06],"Mistral-7B-Instruct-v0.1":[14.46,18.6,6.26,65.79,20.08],"BioMistral-7B-DARE":[18.81,22.92,8.93,68.93,22.65],"Mixtral-8x7B-Instruct-v0.1":[17.44,23.39,7.7,68.56,19.51],"Llama3-OpenBioLLM-8B":[10.82,13.62,4.03,64.14,15.67],"Meta-Llama-3-8B-Instruct":[32.2,36.49,16.37,72.74],"Llama3-OpenBioLLM-70B":[22.7,28.52,9.87,71.45,25.32],"Meta-Llama-3-70B-Instruct":[25.43,33.16,13.01,73,29.12],"Mixtral-8x22B-Instruct-v0.1":[17.97,22.6,8.17,69.29,22.31]},MedNLI:{"zephyr-7b-beta":[68.45],"Mistral-7B-Instruct-v0.1":[64.79],"BioMistral-7B-DARE":[66.76],"Mixtral-8x7B-Instruct-v0.1":[76.48],"Llama3-OpenBioLLM-8B":[44.93],"Meta-Llama-3-8B-Instruct":[74.08],"Llama3-OpenBioLLM-70B":[80.85],"Meta-Llama-3-70B-Instruct":[79.37],"Mixtral-8x22B-Instruct-v0.1":[84.93]}},b=p([{name:"MeDiSumCode",metrics:["EM F1","AP F1","Valid Codes"],results:u.value.map(t=>({label:t,color:B[t],value:M.MeDiSumCode[t]||[]}))},{name:"MeDiSumQA",metrics:["R-L","R-1","R-2","BERT F1","UMLS F1"],results:u.value.map(t=>({label:t,color:B[t],value:M.MeDiSumQA[t]||[]}))},{name:"MeQSum",metrics:["R-L","R-1","R-2","BERT F1"],results:u.value.map(t=>({label:t,color:B[t],value:M.MeQSum[t]||[]}))},{name:"Problem Summary",metrics:["R-L","R-1","R-2","BERT F1","UMLS F1"],results:u.value.map(t=>({label:t,color:B[t],value:M["Problem Summary"][t]||[]}))},{name:"MedNLI",metrics:["Accuracy"],results:u.value.map(t=>({label:t,color:B[t],value:M.MedNLI[t]||[]}))}]),y=p(0),m=z({get:()=>b.value[y.value],set:t=>{const s=b.value.indexOf(t);y.value=s>=0?s:0}}),h=p(b.value[y.value].metrics[0]);I(m,t=>{h.value=t.metrics[0]}),I(y,()=>{h.value=b.value[y.value].metrics[0]});const F=()=>{d=getComputedStyle(document.documentElement),d.getPropertyValue("--text-color"),L=d.getPropertyValue("--text-color-secondary"),w=d.getPropertyValue("--surface-border")},E=(t,s)=>{let i=[],n=[];if(v.value==null)return{labels:i,datasets:n};let r=[],l=[];for(var o of t.results)if(v.value.includes(o.label)&&o.value.length>0){i.push(o.label);let c=t.metrics.findIndex(x=>x==s);r.push(o.value[c]),l.push(o.color)}return n.push({label:s,data:r,backgroundColor:l}),{labels:i,datasets:n}},T=()=>{let t=[],s=[];if(v.value==null)return{labels:t,datasets:s};let i=[],n=[],r=[];for(let[l,o]of Object.entries(S))v.value.includes(l)&&o.length>0&&(t.push(l),r.push(B[l]),i.push(o[0]),o.length>1?n.push(o[1]):n.push(0));return{labels:t,datasets:[{label:"Level 1 Score",data:i,backgroundColor:r},{label:"Level 2 Score",data:n,backgroundColor:r}]}},O=p({plugins:{legend:{display:!1}},scales:{x:{ticks:{color:L,font:{weight:500}},grid:{display:!1,drawBorder:!1}},y:{ticks:{color:L},grid:{color:w,drawBorder:!1}}}}),P=p({plugins:{legend:{display:!1}},indexAxis:"y"});return I(k.theme,()=>{F()},{immediate:!0}),(t,s)=>{const i=g("Button"),n=g("Divider"),r=g("MultiSelect"),l=g("Chart"),o=g("Dropdown"),c=g("TabPanel"),x=g("TabView");return R(),q("div",U,[e("div",K,[e("div",H,[X,J,G,e("a",W,[a(i,{label:"Code",icon:"pi pi-github",severity:"contrast"})]),e("a",Y,[a(i,{label:"Paper",icon:"pi pi-book",severity:"contrast"})])]),e("div",Z,[$,e("p",ee,[a(n),C(" The medical evaluation of LLMs has primarily relied on datasets consisting of multiple choice questions from sources such as MMLU, MEDQA, MedMCQA, and synthetic questions derived from PubMed articles. However, this approach overlooks a significant aspect: none of these evaluations utilize clinical text. The distinction is crucial because, despite the reliance on extensive medical knowledge for these quizzes, the format and content of clinical documents are markedly different. ")]),te,ae,se]),e("div",ne,[oe,a(n),a(r,{class:"w-full",modelValue:v.value,"onUpdate:modelValue":s[0]||(s[0]=_=>v.value=_),display:"chip",options:u.value,placeholder:"Select Models",filter:""},null,8,["modelValue","options"])]),e("div",ie,[le,a(n),a(l,{type:"bar",data:T(m.value,h.value),options:P.value,plugins:t.barPlugins},null,8,["data","options","plugins"])]),e("div",re,[ce,a(n),e("div",de,[e("h5",ue,A(m.value.name)+" ("+A(h.value)+")",1),me,a(o,{class:"mx-3",modelValue:m.value,"onUpdate:modelValue":s[1]||(s[1]=_=>m.value=_),options:b.value,optionLabel:"name",placeholder:"Select Task"},null,8,["modelValue","options"]),a(o,{modelValue:h.value,"onUpdate:modelValue":s[2]||(s[2]=_=>h.value=_),options:m.value.metrics,placeholder:"Select Metric"},null,8,["modelValue","options"])]),a(l,{type:"bar",data:E(m.value,h.value),options:O.value,plugins:t.barPlugins},null,8,["data","options","plugins"])]),e("div",he,[pe,a(n),a(x,null,{default:f(()=>[a(c,{header:"MeDiSumCode"},{default:f(()=>[ge]),_:1}),a(c,{header:"MeDiSumQA"},{default:f(()=>[fe]),_:1}),a(c,{header:"MeQSum"},{default:f(()=>[ve,Be]),_:1}),a(c,{header:"Problem Summary"},{default:f(()=>[Me,be]),_:1}),a(c,{header:"MedNLI"},{default:f(()=>[ye,_e]),_:1}),a(c,{header:"LongHealth"},{default:f(()=>[Le,xe]),_:1})]),_:1})]),e("div",Ie,[Ce,a(n),we]),e("div",De,[Ae,a(n),ke]),Se])])}}};export{Te as default};
