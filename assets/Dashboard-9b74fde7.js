import{u as G,c as R,r as p,w as T,a as h,o as Y,b as $,d as e,e as s,f as u,m as O,g as S,i as Z,t as z,h as l,j as ee}from"./index-d8dc74b2.js";const te="/layout/images/clue-logo.png",ae="/layout/images/ume.png",se="/layout/images/NVIDIA_logo.svg",ne={class:"w-full h-full flex flex-column align-items-center"},ie={class:"w-full lg:w-7"},le={class:"card flex flex-wrap gap-3 justify-content-center header"},oe=ee('<h1 style="font-family:&#39;Google Sans&#39;, sans-serif;"><span style="font-family:&#39;EB Garamond&#39;;">CLUE</span>: A Clinical Language Understanding Evaluation for LLMs</h1><img src="'+te+'" alt="Image Description 1" class="clue-logo"><h4><span class="white-space-nowrap">Amin Dada</span>, <span class="white-space-nowrap">Marie Bauer</span>, <span class="white-space-nowrap">Amanda Butler Contreras</span>, <span class="white-space-nowrap">Osman Alperen Koraş</span>, <span class="white-space-nowrap">Constantin Marc Seibold</span>, <span class="white-space-nowrap">Kaleb E Smith</span>, <span class="white-space-nowrap">Jens Kleesiek</span></h4>',3),re={href:"https://github.com/TIO-IKIM/CLUE",target:"_blank",rel:"noopener noreferrer"},ce={href:"https://arxiv.org/abs/2404.04067",target:"_blank",rel:"noopener noreferrer"},ue=e("h3",{class:"m-0"},"Motivation",-1),de=e("span",{style:{"font-family":"'EB Garamond'"}},"CLUE",-1),me=e("p",null,[l(" Clinical texts are known for their irregular structure, abundant jargon, and region-specific abbreviations, making them substantially different from the concise quiz questions used in standard datasets. Recent studies have also identified significant data contamination in LLM evaluations, raising concerns about the reliability of previous results. In response, "),e("span",{style:{"font-family":"'EB Garamond'"}},"CLUE"),l(" is focusing on creating more robust and realistic medical evaluation methods for LLMs. This initiative aims to improve their integrity and real-world applicability, proposing new strategies for more effective testing. ")],-1),pe=e("h3",{class:"m-0"},"Key Takeaways",-1),he=e("ul",{class:"custom-style mb-0"},[e("li",null,"Specialized biomedical LLMs do not show performance advantages over general purpose LLMs."),e("li",null,"Conversely, certain models adapted for the medical domain show inferior performance compared to their non-specialized counterparts."),e("li",null,"Context length is an important factor, as clinical documents quickly exceed a length of 4k tokens. When few-shot learning is applied, context lengths of 8k tokens are quickly no longer sufficient."),e("li",null," It is possible to generate new data sets for evaluation on the basis of clinical documents that contain a summary, such as discharge summaries. We think that this represents an opportunity to counteract benchmark contamination by applying this approach to private clinical data. ")],-1),fe=e("h3",{class:"m-0"},"Task Descriptions",-1),ve=e("p",{class:"m-0"}," MeDiSumCode requires coding discharge summaries by assigning International Classification of Diseases (ICD-10) codes to diagnoses and procedures. The task challenges language models to extract diagnoses from complex clinical texts and match them with the correct ICD-10 codes, which include over 70,000 options and necessitate a thorough knowledge of the coding system. Additionally, it tests the models' ability to combine diagnosis identification with an understanding of the ICD-10 structure to predict codes effectively. ",-1),ge=e("p",{class:"m-0"},[l(" MeDiSumQA is a medical QA dataset derived from discharge summaries in the MIMIC IV database, designed to generate question-answer pairs about patient hospital stays using a structured Questions Under Discussion framework. The process includes parsing discharge letters, generating questions with minimal answer leakage through similarity metrics, and emphasizing clinical reasoning. You can find a more detailed description in our "),e("a",{href:"https://arxiv.org/abs/2404.04067",target:"_blank",rel:"noopener noreferrer"},"paper"),l(". ")],-1),Be=e("p",{class:"m-0"}," MeQSum comprises 1,000 consumer health inquiries from the U.S. National Library of Medicine that have been manually summarized by medical experts. This task challenges models to condense these often verbose and vague inquiries into concise, medically accurate questions. The ability to effectively interpret and summarize patient inquiries, despite not involving clinical documents, is highly valuable for applications in healthcare settings. ",-1),be={style:{"background-color":"#f9fafb","overflow-x":"auto",position:"relative"}},Le=e("p",{class:"m-0"}," This downstream task, involves using the Subjective and Assessment sections of clinical notes, structured by the SOAP principle, to predict a patient's current health problems. The Objective section is excluded from inputs, while the Plan section provides the ground truth for the models' predictions. This task is a form of information extraction that challenges models to identify specific health issues from condensed, preprocessed clinical text. ",-1),Me={style:{"background-color":"#f9fafb","overflow-x":"auto",position:"relative"}},ye=e("p",{class:"m-0"}," MedNLI, derived from the MIMIC III dataset, features clinical notes from which sentences are extracted and used as premises for clinicians to generate three corresponding hypotheses: contradictory, neutral, and entailed. The core task involves predicting the logical relationship—whether it is contradiction, neutrality, or entailment—between each premise and its hypotheses. With its focus on short input lengths, MedNLI effectively measures a model's clinical reasoning capabilities without the complexity of processing long text inputs. ",-1),xe={style:{"background-color":"#f9fafb","overflow-x":"auto",position:"relative"}},_e=e("p",{class:"m-0"}," The LongHealth dataset features 20 fictional patient records to test language models on their ability to handle extended texts, as these models typically underperform with longer inputs. It tasks models with answering multiple-choice questions, structured into three distinct evaluation sub-tasks. These tasks progressively increase in complexity by first focusing on relevant documents, then introducing extraneous information to filter out, and finally testing the model's ability to identify unprovided information. In contrast to the original paper, we have set the maximum context length to 8k tokens. ",-1),Ie={style:{"background-color":"#f9fafb","overflow-x":"auto",position:"relative"}},ke={id:"model-select",class:"card w-full my-3"},we=e("h3",null,"Model Selection",-1),Se=e("p",null,"Select one or more models from the list below to view their benchmark results.",-1),Ae={class:"card w-full my-3"},Ce=e("h3",null,"Average Scores",-1),De=e("p",null,[l(" We provide two averaged scores: Level 1 and Level 2. Level 1 includes the tasks MedNLI, MeQSum and Problem Summary, which feature shorter examples. Level 2 includes the remaining tasks: MeDiSumCode, MeDiSumQA and LongHealth. Further details can be found in our "),e("a",{href:"https://arxiv.org/abs/2404.04067",target:"_blank",rel:"noopener noreferrer"},"paper"),l(". ")],-1),Pe={class:"card w-full"},Te=e("h3",null,"Individual Task Scores",-1),Oe=e("p",null,"Explore detailed performance results for each model by selecting a specific task and metric.",-1),Ee={class:"w-full mb-3"},Fe={class:"mb-3"},Je=e("div",{class:"flex-grow-1"},null,-1),Re={class:"card w-full my-3"},ze=e("h3",null,"BibTeX",-1),Ve={class:"section",id:"BibTeX"},Ne={class:"container is-max-desktop content",style:{position:"relative"}},qe=e("p",null,"If you find our work useful, please cite our paper:",-1),Qe={style:{"background-color":"#f9fafb","overflow-x":"auto",position:"relative"}},Ue={class:"card w-full my-3"},je=e("h3",null,"Contact",-1),He=e("p",null,[l("Please reach out to "),e("a",{class:"link blue",href:"mailto:amin.dada@uk-essen.de"},"Amin Dada"),l(" if you have any comments, questions, or suggestions.")],-1),Ke=e("div",{class:"flex flex-column md:flex-row align-items-center justify-content-center"},[e("img",{src:ae,class:"ume-logo"}),e("img",{src:se,class:"nvidia-logo"})],-1),Ye={__name:"Dashboard",setup(We){const{layoutConfig:V}=G();let b=getComputedStyle(document.documentElement);b.getPropertyValue("--text-color");let A=b.getPropertyValue("--text-color-secondary"),E=b.getPropertyValue("--surface-border");const N=R(()=>window.innerWidth<720),F=["#8CB0A4","#C6B786","#8C8CFF","#FF9E7A","#BFA5A5","#9698D2","#B3B9D7","#A2B8B5","#D5B1B0","#BEB888","#9797FF","#FFBFAA","#81BAA4","#A978FF","#A8C697","#CCD488","#FFA78D","#AFC48F","#D0A9B3","#CBB99D","#8290D2","#FF8A8A","#978878","#90B2D7","#7AB8A1","#FFB985","#D4B3A5","#8BA78B","#8AA3C7","#9797E0"],f=p(["llama2-7b","meditron-7b","llama2-7b-hf","Mistral-7B-v0.1","internistai/base-7b-v0.2","BioMistral-7B","BioMistral-7B-DARE","Mistral-7B-Instruct-v0.1","zephyr-7b-beta","Mistral-7B-Instruct-v0.2","Meta-Llama-3-8B-Instruct","Llama3-OpenBioLLM-8B","JSL-Med-Sft-Llama-3-8B","JSL-MedLlama-3-8B-v1.0","JSL-MedLlama-3-8B-v2.0","Llama3-Aloe-8B-Alpha","Phi-3-mini-128k-instruct","Mixtral-8x7B-v0.1","Mixtral-8x7B-Instruct-v0.1","Llama-2-70b","meditron-70b","ClinicalCamel-70B","Llama-2-70b-chat","Meta-Llama-3-70B-Instruct","Llama3-OpenBioLLM-70B","Mixtral-8x22B-Instruct-v0.1"]),I=(()=>[/Android/i,/webOS/i,/iPhone/i,/iPad/i,/iPod/i,/BlackBerry/i,/Windows Phone/i].some(a=>navigator.userAgent.match(a)))(),d=I?p(["Llama3-OpenBioLLM-70B","Meta-Llama-3-70B-Instruct"]):p(["Llama3-OpenBioLLM-8B","Meta-Llama-3-8B-Instruct","Llama3-OpenBioLLM-70B","Meta-Llama-3-70B-Instruct"]),v=f.value.reduce((t,a,i)=>(t[a]=F[i%F.length],t),{}),k={"Mistral-7B-v0.1":["Mistral-7B-v0.1","internistai/base-7b-v0.2","zephyr-7b-beta"],"Mistral-7B-Instruct-v0.1":["Mistral-7B-Instruct-v0.1","BioMistral-7B","BioMistral-7B-DARE"],"Mistral-7B-Instruct-v0.2":["Mistral-7B-Instruct-v0.2"],"Meta-Llama-3-8B-Instruct":["Meta-Llama-3-8B-Instruct","Llama3-OpenBioLLM-8B","JSL-Med-Sft-Llama-3-8B","JSL-MedLlama-3-8B-v1.0","JSL-MedLlama-3-8B-v2.0","Llama3-Aloe-8B-Alpha"],"Phi-3-mini-128k-instruct":["Phi-3-mini-128k-instruct"],"Mixtral-8x7B-Instruct-v0.1":["Mixtral-8x7B-Instruct-v0.1"],"Meta-Llama-3-70B-Instruct":["Meta-Llama-3-70B-Instruct","Llama3-OpenBioLLM-70B"],"Mixtral-8x22B-Instruct-v0.1":["Mixtral-8x22B-Instruct-v0.1"]},w={"llama2-7b":[20.11],"meditron-7b":[11.46],"llama2-7b-hf":[36.94],"Mistral-7B-v0.1":[33.99,9.55],"internistai/base-7b-v0.2":[38.24,28.64],"BioMistral-7B":[40.57,23.83],"BioMistral-7B-DARE":[43.24,25.82],"Mistral-7B-Instruct-v0.1":[40.31,23.12],"zephyr-7b-beta":[42.69,28.17],"Mistral-7B-Instruct-v0.2":[46.45,38.94],"Meta-Llama-3-8B-Instruct":[48.37,40.61],"Llama3-OpenBioLLM-8B":[33.2,25.44],"JSL-Med-Sft-Llama-3-8B":[19,15.68],"JSL-MedLlama-3-8B-v1.0":[17,29.94],"JSL-MedLlama-3-8B-v2.0":[31.26,18.02],"Llama3-Aloe-8B-Alpha":[42.5,30.31],"Phi-3-mini-128k-instruct":[41.7,24.39],"Mixtral-8x7B-Instruct-v0.1":[47.81,42.57],"Llama-2-70b":[35.17],"meditron-70b":[30.58],"ClinicalCamel-70B":[35.71],"Llama-2-70b-chat":[42.9],"Meta-Llama-3-70B-Instruct":[52.36,56],"Llama3-OpenBioLLM-70B":[47.57,45.56],"Mixtral-8x22B-Instruct-v0.1":[51.88,51.23]},L={MeDiSumCode:{"Mistral-7B-v0.1":[.77,5.32,33.21],"internistai/base-7b-v0.2":[1.87,10.25,53.57],"BioMistral-7B":[1.67,9.92,54.51],"BioMistral-7B-DARE":[1.2,6.66,56.04],"Mistral-7B-Instruct-v0.1":[.57,3.78,37.25],"zephyr-7b-beta":[2.31,12,71.27],"Mistral-7B-Instruct-v0.2":[3.08,18.23,68.76],"Meta-Llama-3-8B-Instruct":[3.95,17.55,61.93],"Llama3-OpenBioLLM-8B":[.84,4.84,51.16],"JSL-Med-Sft-Llama-3-8B":[3.09,14.03,68.37],"JSL-MedLlama-3-8B-v1.0":[1.87,12.8,67.3],"JSL-MedLlama-3-8B-v2.0":[1.41,11.13,64.31],"Llama3-Aloe-8B-Alpha":[1.77,12.78,44.84],"Phi-3-mini-128k-instruct":[.43,3.63,86.24],"Mixtral-8x7B-Instruct-v0.1":[10.49,28.99,82.87],"Llama3-OpenBioLLM-70B":[7.37,20.24,73.65],"Meta-Llama-3-70B-Instruct":[19.65,39.2,93.94],"Mixtral-8x22B-Instruct-v0.1":[15.95,35.96,79.84]},MeDiSumQA:{"Mistral-7B-v0.1":[5.93,7.16,1.59,53.47,7.47],"internistai/base-7b-v0.2":[9.24,11.94,3.23,61.82,13],"BioMistral-7B":[14.65,17.81,5.46,59.01,16.88],"BioMistral-7B-DARE":[17.01,20.87,6.92,65.17,18.45],"Mistral-7B-Instruct-v0.1":[16.62,21.34,6.95,65.68,16.77],"zephyr-7b-beta":[13.02,17.68,4.98,64.08,13.92],"Mistral-7B-Instruct-v0.2":[21.8,27.47,9.19,68.43,20.3],"Meta-Llama-3-8B-Instruct":[22.44,28.15,9.63,68.62,22.74],"Llama3-OpenBioLLM-8B":[22.89,27.95,10.45,68.7,22.15],"JSL-Med-Sft-Llama-3-8B":[8.85,11.74,3.21,58.38,10.54],"JSL-MedLlama-3-8B-v1.0":[19.06,22.7,7.63,67.29,20.83],"JSL-MedLlama-3-8B-v2.0":[19.13,24.15,8.56,67.41,20.49],"Llama3-Aloe-8B-Alpha":[11.03,14.97,4.57,63.87,12.68],"Phi-3-mini-128k-instruct":[18.47,22.45,6.94,65.72,16.52],"Mixtral-8x7B-Instruct-v0.1":[20.72,26.4,8.96,67.74,20.25],"Llama3-OpenBioLLM-70B":[21.85,27.8,9.54,68.43,22.47],"Meta-Llama-3-70B-Instruct":[26.2,32.5,11.93,70.24,25.78],"Mixtral-8x22B-Instruct-v0.1":[21.81,27.75,9.42,68.51,22.7]},MeQSum:{"llama2-7b":[7.16,8.58,2.89,37.5],"meditron-7b":[6.17,7.47,2.53,40.53],"llama2-7b-hf":[36.45,39.99,18.13,75.98],"Mistral-7B-v0.1":[6.28,7.86,2.49,44.6],"internistai/base-7b-v0.2":[7.65,9.54,3.44,40.34],"BioMistral-7B":[25.89,28.46,13.31,67.93],"BioMistral-7B-DARE":[26.16,29.69,13.49,68.68],"Mistral-7B-Instruct-v0.1":[21.85,25.26,11.17,66.15],"zephyr-7b-beta":[25.66,29.81,12.33,68.85],"Mistral-7B-Instruct-v0.2":[33.54,37.47,16.61,73.47],"Meta-Llama-3-8B-Instruct":[32.2,36.49,16.37,72.74],"Llama3-OpenBioLLM-8B":[26.21,29.41,14.03,62.39],"JSL-Med-Sft-Llama-3-8B":[6.57,7.73,2.99,31.7],"JSL-MedLlama-3-8B-v1.0":[6.02,7.19,2.43,41.78],"JSL-MedLlama-3-8B-v2.0":[30.93,34.54,16.31,71.03],"Llama3-Aloe-8B-Alpha":[23.78,27.02,12.31,66.01],"Phi-3-mini-128k-instruct":[32.05,35.85,15.81,73.01],"Mixtral-8x7B-v0.1":[9.36,10.35,4.3,31.04],"Mixtral-8x7B-Instruct-v0.1":[32.47,36.38,16.86,72.8],"Llama-2-70b":[3.75,4.15,1.05,33.59],"meditron-70b":[2.95,3.24,.76,31.27],"ClinicalCamel-70B":[16.96,18.8,8.49,49.3],"Llama-2-70b-chat":[34.91,38.81,18.48,74.37],"Meta-Llama-3-70B-Instruct":[36.57,40.2,19.3,75.74],"Llama3-OpenBioLLM-70B":[30.72,34.31,15.55,71.99],"Mixtral-8x22B-Instruct-v0.1":[36.15,39.94,19.45,75.02]},"Problem Summary":{"llama2-7b":[5.97,7.35,2.11,59.45,9.06],"meditron-7b":[6.49,7.87,2.59,59.57,12.52],"llama2-7b-hf":[17.43,22.25,6.93,66.33,21.62],"Mistral-7B-v0.1":[7.07,8.99,3.17,60.74,15.67],"internistai/base-7b-v0.2":[13.11,16.79,5.63,62.94,17.22],"BioMistral-7B":[16.9,20.89,8.4,59.03,20.12],"BioMistral-7B-DARE":[18.81,22.92,8.93,68.93,22.65],"Mistral-7B-Instruct-v0.1":[14.46,18.6,6.26,65.79,20.08],"zephyr-7b-beta":[14.81,19.91,5.98,67.39,19.2],"Mistral-7B-Instruct-v0.2":[19.57,25.59,8.92,69.64,22.07],"Meta-Llama-3-8B-Instruct":[22.7,28.52,9.87,71.45,25.32],"Llama3-OpenBioLLM-8B":[10.82,13.62,4.03,64.14,15.67],"JSL-Med-Sft-Llama-3-8B":[4.59,5.75,1.61,57.69,8.75],"JSL-MedLlama-3-8B-v1.0":[12.87,15.4,4.73,65.18,18.42],"JSL-MedLlama-3-8B-v2.0":[8.16,10.61,2.94,64.48,13.17],"Llama3-Aloe-8B-Alpha":[9.47,12.3,4.06,65.56,15],"Phi-3-mini-128k-instruct":[19.8,23.72,8.47,70.27,21.06],"Mixtral-8x7B-v0.1":[7.26,9.37,3.14,60.54,11.94],"Mixtral-8x7B-Instruct-v0.1":[17.44,23.39,7.7,68.56,19.51],"Llama-2-70b":[7.13,8.77,3.15,59.61,14.34],"meditron-70b":[7.22,8.91,3.21,60.08,13.91],"ClinicalCamel-70B":[8.62,10.83,3.71,59.94,12.34],"Llama-2-70b-chat":[14.43,19.81,6.14,65.07,21.37],"Meta-Llama-3-70B-Instruct":[25.43,33.16,13.01,73,29.12],"Llama3-OpenBioLLM-70B":[12.1,16.67,5.58,66.51,17.74],"Mixtral-8x22B-Instruct-v0.1":[17.97,22.6,8.17,69.29,22.31]},MedNLI:{"llama2-7b":[29.51],"meditron-7b":[2.39],"llama2-7b-hf":[41.27],"Mistral-7B-v0.1":[67.54],"internistai/base-7b-v0.2":[76.34],"BioMistral-7B":[62.75],"BioMistral-7B-DARE":[66.76],"Mistral-7B-Instruct-v0.1":[64.79],"zephyr-7b-beta":[68.45],"Mistral-7B-Instruct-v0.2":[69.93],"Meta-Llama-3-8B-Instruct":[74.08],"Llama3-OpenBioLLM-8B":[44.93],"JSL-Med-Sft-Llama-3-8B":[29.08],"JSL-MedLlama-3-8B-v1.0":[13.31],"JSL-MedLlama-3-8B-v2.0":[35.7],"Llama3-Aloe-8B-Alpha":[73.94],"Phi-3-mini-128k-instruct":[57.25],"Mixtral-8x7B-v0.1":[80.14],"Mixtral-8x7B-Instruct-v0.1":[76.48],"Llama-2-70b":[76.27],"meditron-70b":[63.52],"ClinicalCamel-70B":[64.65],"Llama-2-70b-chat":[61.69],"Meta-Llama-3-70B-Instruct":[79.37],"Llama3-OpenBioLLM-70B":[80.85],"Mixtral-8x22B-Instruct-v0.1":[84.93]},LongHealth:{"Mistral-7B-v0.1":[9.55,.6,.55,.15],"internistai/base-7b-v0.2":[28.64,52.75,30.6,49.2],"BioMistral-7B":[23.83,38.05,34.25,7.8],"BioMistral-7B-DARE":[25.82,46,40.85,4.6],"Mistral-7B-Instruct-v0.1":[23.12,45.75,40.65,3.65],"zephyr-7b-beta":[28.17,42.9,30.5,26.35],"Mistral-7B-Instruct-v0.2":[38.94,67.2,62.4,42.45],"Meta-Llama-3-8B-Instruct":[40.61,68.3,66.55,56.25],"Llama3-OpenBioLLM-8B":[25.44,37.55,41.75,1.55],"JSL-Med-Sft-Llama-3-8B":[15.68,0,0,0],"JSL-MedLlama-3-8B-v1.0":[29.94,52.85,52.05,.05],"JSL-MedLlama-3-8B-v2.0":[18.02,1.1,.3,.05],"Llama3-Aloe-8B-Alpha":[30.31,66.75,63.3,19.05],"Phi-3-mini-128k-instruct":[24.39,27.25,23.9,0],"Mixtral-8x7B-Instruct-v0.1":[42.57,76.5,73.65,24.2],"Llama3-OpenBioLLM-70B":[45.56,80.2,75.6,62.9],"Meta-Llama-3-70B-Instruct":[56,81.65,77.9,91.7],"Mixtral-8x22B-Instruct-v0.1":[51.23,79,73.9,86.3]}},M=p([{name:"MeDiSumCode",metrics:["EM F1","AP F1","Valid Codes"],results:f.value.map(t=>({label:t,color:v[t],value:L.MeDiSumCode[t]||[]}))},{name:"MeDiSumQA",metrics:["R-L","R-1","R-2","BERT F1","UMLS F1"],results:f.value.map(t=>({label:t,color:v[t],value:L.MeDiSumQA[t]||[]}))},{name:"MeQSum",metrics:["R-L","R-1","R-2","BERT F1"],results:f.value.map(t=>({label:t,color:v[t],value:L.MeQSum[t]||[]}))},{name:"Problem Summary",metrics:["R-L","R-1","R-2","BERT F1","UMLS F1"],results:f.value.map(t=>({label:t,color:v[t],value:L["Problem Summary"][t]||[]}))},{name:"MedNLI",metrics:["Accuracy"],results:f.value.map(t=>({label:t,color:v[t],value:L.MedNLI[t]||[]}))},{name:"LongHealth",metrics:["Task 1 Accuracy","Task 2 Accuracy","Task 3 Accuracy"],results:f.value.map(t=>({label:t,color:v[t],value:L.LongHealth[t]||[]}))}]),y=p(0),g=R({get:()=>M.value[y.value],set:t=>{const a=M.value.indexOf(t);y.value=a>=0?a:0}}),B=p(M.value[y.value].metrics[0]);T(g,t=>{B.value=t.metrics[0]}),T(y,()=>{B.value=M.value[y.value].metrics[0]});const q=()=>{b=getComputedStyle(document.documentElement),b.getPropertyValue("--text-color"),A=b.getPropertyValue("--text-color-secondary"),E=b.getPropertyValue("--surface-border")},Q=(t,a)=>{let i=[],n=[];if(d.value==null)return{labels:i,datasets:n};let r=[],m=[];for(var o of t.results)if(d.value.includes(o.label)&&o.value.length>0){i.push(o.label);let C=t.metrics.findIndex(D=>D==a);r.push(o.value[C]),m.push(o.color)}return n.push({label:a,data:r,backgroundColor:m}),{labels:i,datasets:n}},U=()=>{let t=[],a=[];if(d.value==null)return{labels:t,datasets:a};for(let i in k)for(let n of k[i]){let r={label:n,data:[],backgroundColor:v[n],showLine:!1,pointRadius:5},m=w[n];m.length==2&&d.value.includes(n)&&r.data.push(m),r.data.length>0&&a.push(r)}for(let i in k)for(let n of k[i])d.value.includes(i)&&d.value.includes(n)&&i!=n&&a.push({type:"line",data:[w[i],w[n]],label:"ignore"});return{datasets:a}},j=()=>{let t=[],a=[];if(d.value==null)return{labels:t,datasets:a};let i=[],n=[],r=[];for(let[m,o]of Object.entries(w))d.value.includes(m)&&o.length>0&&(t.push(m),r.push(v[m]),i.push(o[0]),o.length>1?n.push(o[1]):n.push(0));return{labels:t,datasets:[{label:"Level 1 Score",data:i,backgroundColor:r},{label:"Level 2 Score",data:n,backgroundColor:r}]}},H=p({plugins:{legend:{display:!1}},scales:{x:{ticks:{color:A,font:{weight:500}},grid:{display:!1,drawBorder:!1}},y:{ticks:{color:A},grid:{color:E,drawBorder:!1}}}}),K=p({plugins:{legend:{display:!1}},indexAxis:N.value?"x":"y"}),W=p({plugins:{tooltip:{callbacks:{label:function(t){return t.dataset.label}},filter:function(t,a){return console.log(t),a==0&&t.dataset.label!="ignore"}},legend:{display:!0,labels:{filter:function(t,a){return t.text!="ignore"}}},annotation:{annotations:{line:{arrowHeads:{end:{display:!0}}}}}},scales:{x:{title:{text:"Average Level 1 Score",display:!0}},y:{title:{text:"Average Level 2 Score",display:!0}}}});T(V.theme,()=>{q()},{immediate:!0});const X=t=>{const i=document.querySelector(t);if(i){const r=i.getBoundingClientRect().top+window.pageYOffset-70;window.scrollTo({top:r,behavior:"smooth"})}},x=p(null),_=t=>{const a=t.target.nextElementSibling;if(a){const i=a.innerText;navigator.clipboard.writeText(i)}};return(t,a)=>{const i=h("Button"),n=h("Divider"),r=h("Panel"),m=h("divider"),o=h("TabPanel"),C=h("TabView"),D=h("MultiSelect"),P=h("Chart"),J=h("Dropdown");return Y(),$("div",ne,[e("div",ie,[e("div",le,[oe,e("a",re,[s(i,{label:"Code",icon:"pi pi-github",severity:"contrast"})]),e("a",ce,[s(i,{label:"Paper",icon:"pi pi-book",severity:"contrast"})]),s(i,{label:"Results",icon:"pi pi-chart-bar",onClick:a[0]||(a[0]=c=>X("#model-select")),severity:"contrast"})]),s(r,O({class:"w-full my-3 p-3"},{...S(I)?{collapsed:!0}:{},toggleable:!0}),{header:u(()=>[ue]),default:u(()=>[e("p",null,[s(n),l(" Biomedical LLMs promise significant advancements in patient care, yet their real-world evaluation remains lacking. Current assessments focus primarily on medical knowledge via constructed questions, which do not fully capture the complexity and diversity of clinical tasks. Furthermore, the rapid evolution of LLMs makes it challenging to choose the most suitable models for healthcare applications. To address these issues, "),de,l(" is a comprehensive and standardized framework to assess the performance of specialized biomedical and advanced general-domain LLMs in practical healthcare tasks. ")]),me]),_:1},16),s(r,O({class:"w-full my-3 p-3"},{...S(I)?{collapsed:!0}:{},toggleable:!0}),{header:u(()=>[pe]),default:u(()=>[s(m),he]),_:1},16),s(r,O({class:"w-full my-3 p-3"},{...S(I)?{collapsed:!0}:{},toggleable:!0}),{header:u(()=>[fe]),default:u(()=>[s(C,{class:"mt-3"},{default:u(()=>[s(o,{header:"MeDiSumCode"},{default:u(()=>[ve]),_:1}),s(o,{header:"MeDiSumQA"},{default:u(()=>[ge]),_:1}),s(o,{header:"MeQSum"},{default:u(()=>[Be,e("pre",be,[l(""),e("button",{onClick:a[1]||(a[1]=c=>_(c)),class:"copy-btn pi pi-copy",style:{position:"absolute",top:"5px",right:"5px"}}),l(`
`),e("code",{ref_key:"bibtex",ref:x},`@inproceedings{abacha2019summarization,
  title={On the summarization of consumer health questions},
  author={Abacha, Asma Ben and Demner-Fushman, Dina},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2228--2234},
  year={2019}
}`,512)])]),_:1}),s(o,{header:"Problem Summary"},{default:u(()=>[Le,e("pre",Me,[l(""),e("button",{onClick:a[2]||(a[2]=c=>_(c)),class:"copy-btn pi pi-copy",style:{position:"absolute",top:"5px",right:"5px"}}),l(`
`),e("code",{ref_key:"bibtex",ref:x},`@inproceedings{gao2022summarizing,
  title={Summarizing patients’ problems from hospital progress notes using pre-trained sequence-to-sequence models},
  author={Gao, Yanjun and Miller, Timothy and Xu, Dongfang and Dligach, Dmitriy and Churpek, Matthew M and Afshar, Majid},
  booktitle={Proceedings of COLING. International Conference on Computational Linguistics},
  volume={2022},
  pages={2979},
  year={2022},
  organization={NIH Public Access}
}`,512)])]),_:1}),s(o,{header:"MedNLI"},{default:u(()=>[ye,e("pre",xe,[l(""),e("button",{onClick:a[3]||(a[3]=c=>_(c)),class:"copy-btn pi pi-copy",style:{position:"absolute",top:"5px",right:"5px"}}),l(`
`),e("code",{ref_key:"bibtex",ref:x},`@article{romanov2018lessons,
	title = {Lessons from Natural Language Inference in the Clinical Domain},
	url = {http://arxiv.org/abs/1808.06752},
	abstract = {State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce {MedNLI} - a dataset annotated by doctors, performing a natural language inference task ({NLI}), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. {SNLI}) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.},
	journaltitle = {arXiv:1808.06752 [cs]},
	author = {Romanov, Alexey and Shivade, Chaitanya},
	urldate = {2018-08-27},
	date = {2018-08-21},
	eprinttype = {arxiv},
	eprint = {1808.06752},
}`,512)])]),_:1}),s(o,{header:"LongHealth"},{default:u(()=>[_e,e("pre",Ie,[l(""),e("button",{onClick:a[4]||(a[4]=c=>_(c)),class:"copy-btn pi pi-copy",style:{position:"absolute",top:"5px",right:"5px"}}),l(`
`),e("code",{ref_key:"bibtex",ref:x},`@article{adams2024longhealth,
  title={LongHealth: A Question Answering Benchmark with Long Clinical Documents},
  author={Adams, Lisa and Busch, Felix and Han, Tianyu and Excoffier, Jean-Baptiste and Ortala, Matthieu and L{\\"o}ser, Alexander and Aerts, Hugo JWL and Kather, Jakob Nikolas and Truhn, Daniel and Bressem, Keno},
  journal={arXiv preprint arXiv:2401.14490},
  year={2024}
}`,512)])]),_:1})]),_:1})]),_:1},16),e("div",ke,[we,Se,s(n),s(D,{class:"w-full",modelValue:S(d),"onUpdate:modelValue":a[5]||(a[5]=c=>Z(d)?d.value=c:null),display:"chip",options:f.value,placeholder:"Select Models",filter:""},null,8,["modelValue","options"])]),e("div",Ae,[Ce,s(n),De,s(P,{type:"scatter",data:U(g.value,B.value),options:W.value},null,8,["data","options"]),s(n),s(P,{type:"bar",data:j(g.value,B.value),options:K.value,plugins:t.barPlugins},null,8,["data","options","plugins"])]),e("div",Pe,[Te,Oe,s(n),e("div",Ee,[e("h5",Fe,z(g.value.name)+" ("+z(B.value)+")",1),Je,s(J,{modelValue:g.value,"onUpdate:modelValue":a[6]||(a[6]=c=>g.value=c),options:M.value,optionLabel:"name",placeholder:"Select Task"},null,8,["modelValue","options"]),s(J,{modelValue:B.value,"onUpdate:modelValue":a[7]||(a[7]=c=>B.value=c),options:g.value.metrics,placeholder:"Select Metric"},null,8,["modelValue","options"])]),s(P,{type:"bar",data:Q(g.value,B.value),options:H.value,plugins:t.barPlugins},null,8,["data","options","plugins"])]),e("div",Re,[ze,s(n),e("section",Ve,[e("div",Ne,[qe,e("pre",Qe,[l("                            "),e("button",{onClick:a[8]||(a[8]=c=>_(c)),class:"copy-btn pi pi-copy",style:{position:"absolute",top:"5px",right:"5px"}}),l(`
`),e("code",{ref_key:"bibtex",ref:x},`@article{dada2024clue,
  title={CLUE: A Clinical Language Understanding Evaluation for LLMs},
  author={Dada, Amin and Bauer, Marie and Contreras, Amanda Butler and Kora{\\c{s}}, Osman Alperen and Seibold, Constantin Marc and Smith, Kaleb E and Kleesiek, Jens},
  journal={arXiv preprint arXiv:2404.04067},
  year={2024}
}`,512)])])])]),e("div",Ue,[je,s(n),He]),Ke])])}}};export{Ye as default};
